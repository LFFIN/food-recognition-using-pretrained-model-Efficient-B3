{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This is the final model we worked on after trying several pretrained models  and. We found that EfficientNet-B3 gave the best results, and after further tuning, we reached an F1-score of 0.51049, slightly after the competition ended.**\n",
        "**The code has been updated constantly throughout the work, so this is mainly the final version of the code and the steps we took to build this model.**"
      ],
      "metadata": {
        "id": "mHqEZ5rE8Hsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU5aJ0xf2vu7",
        "outputId": "a614215d-51ad-4e49-ea86-9a1c9bcadfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcVEV-1IyV76"
      },
      "outputs": [],
      "source": [
        "#2. Importing Required Libraries and setting file path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models.efficientnet import efficientnet_b3, EfficientNet_B3_Weights\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#3. Setting File Paths and Parameters\n",
        "BASE_DIR = \"/content/drive/MyDrive/food-recognition\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"images_train\")\n",
        "TEST_DIR = os.path.join(BASE_DIR, \"images_test\")\n",
        "LABEL_FILE = os.path.join(BASE_DIR, \"train_onehot.csv\")\n",
        "\n",
        "#setting values\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25\n",
        "THRESHOLD = 0.25\n",
        "FOLDS = 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Label CSV and Process\n",
        "df = pd.read_csv(LABEL_FILE)\n",
        "df['Filename'] = df['Filename'].str.strip()  # removing extra spaces if any occurs\n",
        "\n",
        "#Preparing inputs and targets\n",
        "image_paths = [os.path.join(TRAIN_DIR, fname) for fname in df['Filename']]\n",
        "labels = df.drop(columns='Filename').values.astype(np.float32)\n",
        "num_classes = labels.shape[1]"
      ],
      "metadata": {
        "id": "rGQq8-GB5kaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Dataset Class for Training and Validation\n",
        "\n",
        "class FoodDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file: {self.image_paths[idx]} due to error: {e}\")\n",
        "            return None  # Return None for problematic images ( Since have enouctered few times )\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "OyeV9fad5sEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Data Augmentation and Normalization\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "metadata": {
        "id": "liu-Wgdo51gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Training Function for Each Fold\n",
        "def train_fold(train_loader, val_loader, fold):\n",
        "    model = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "# ✅ Loading previously saved weights to resume from 12 to 25\n",
        "    model_path = os.path.join(BASE_DIR, f\"efficientnet_b3_fold{fold+1}_best.pth\")\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Resuming training from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(12, EPOCHS + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "    for images, targets in tqdm(train_loader, desc=f\"Fold {fold+1} - Epoch {epoch+1}\"):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Running validation after each epoch\n",
        "        val_f1 = evaluate_model(model, val_loader)\n",
        "        print(f\"Fold {fold+1} - Epoch {epoch+1} - Loss: {total_loss / len(train_loader.dataset):.4f} - Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Saving best model per fold\n",
        "        if val_f1 > best_f1:\n",
        "           best_f1 = val_f1\n",
        "           torch.save(model.state_dict(), os.path.join(BASE_DIR, f\"efficientnet_b3_fold{fold+1}_best.pth\"))\n"
      ],
      "metadata": {
        "id": "Pd3lgBz551ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Also Validating function to evaluate F1 on Val Set\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.append(probs)\n",
        "            all_targets.append(targets.numpy())\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    return f1_score(all_targets, (all_preds > THRESHOLD).astype(int), average=\"samples\")"
      ],
      "metadata": {
        "id": "qVLqpEzC51kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then run training and validation across folds manual fold 3 training only,already run 1-12 folds\n",
        "skf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths)):\n",
        "    if fold != 2:\n",
        "        continue\n",
        "\n",
        "train_paths = np.array(image_paths)[train_idx]\n",
        "val_paths = np.array(image_paths)[val_idx]\n",
        "train_labels = labels[train_idx]\n",
        "val_labels = labels[val_idx]\n",
        "\n",
        "train_loader = DataLoader(FoodDataset(train_paths, train_labels, transform), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(FoodDataset(val_paths, val_labels, transform), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "train_fold(train_loader, val_loader, fold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juGhcK5w51n-",
        "outputId": "582aa637-2a76-432d-d2ec-c37f4dbca144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 13: 100%|██████████| 833/833 [1:49:53<00:00,  7.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 13 - Loss: 0.0491 - Val F1: 0.1237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 14: 100%|██████████| 833/833 [12:43<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 14 - Loss: 0.0172 - Val F1: 0.2738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 15: 100%|██████████| 833/833 [12:40<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 15 - Loss: 0.0150 - Val F1: 0.3615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 16: 100%|██████████| 833/833 [12:33<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 16 - Loss: 0.0134 - Val F1: 0.4063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 17: 100%|██████████| 833/833 [12:33<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 17 - Loss: 0.0122 - Val F1: 0.4463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 18: 100%|██████████| 833/833 [12:34<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 18 - Loss: 0.0111 - Val F1: 0.4742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 19: 100%|██████████| 833/833 [12:34<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 19 - Loss: 0.0103 - Val F1: 0.4872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 20: 100%|██████████| 833/833 [12:33<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 20 - Loss: 0.0095 - Val F1: 0.4986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 21: 100%|██████████| 833/833 [12:34<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 21 - Loss: 0.0087 - Val F1: 0.5051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 22: 100%|██████████| 833/833 [12:32<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 22 - Loss: 0.0080 - Val F1: 0.5172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 23: 100%|██████████| 833/833 [13:05<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 23 - Loss: 0.0074 - Val F1: 0.5151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 24: 100%|██████████| 833/833 [13:09<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 24 - Loss: 0.0068 - Val F1: 0.5202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 25: 100%|██████████| 833/833 [13:12<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 25 - Loss: 0.0062 - Val F1: 0.5196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 - Epoch 26: 100%|██████████| 833/833 [13:11<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Epoch 26 - Loss: 0.0057 - Val F1: 0.5180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8-1 defining Dataset Class for Test Data\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, os.path.basename(self.image_paths[idx])"
      ],
      "metadata": {
        "id": "Uomdj_xe5179"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8-2efining Inference Function for Final Predictions\n",
        "def predict_test():\n",
        "# Loading model trained on the last fold\n",
        "    model = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(os.path.join(BASE_DIR, f\"efficientnet_b3_fold{FOLDS}_best.pth\")))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "# Defining transform for test images\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((300, 300)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    test_image_names = sorted(os.listdir(TEST_DIR))\n",
        "    test_image_paths = [os.path.join(TEST_DIR, name) for name in test_image_names]\n",
        "    test_loader = DataLoader(TestDataset(test_image_paths, transform=test_transform), batch_size=32, shuffle=False)\n",
        "\n",
        "    final_preds, filenames = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, names in test_loader:\n",
        "            images = images.to(device)\n",
        "            probs = torch.sigmoid(model(images)).cpu().numpy()\n",
        "            final_preds.append(probs)\n",
        "            filenames.extend(names)\n",
        "\n",
        "\n",
        "# Converting probabilities to binary predictions using threshold\n",
        "    final_binary = (np.vstack(final_preds) > THRESHOLD).astype(int)\n",
        "\n",
        "    return final_binary, filenames\n",
        "\n",
        "#9 Creating submission dataFrame\n",
        "    submission = pd.DataFrame(final_binary, columns=[str(i) for i in range(num_classes)])\n",
        "    submission.insert(0, \"Filename\", filenames)\n",
        "    submission.to_csv(os.path.join(BASE_DIR, \"submission_efficientnet_b3_F3_25.csv\"), index=False)\n",
        "\n",
        "#Running Inference on Test Set and Save it as CSV\n",
        "predict_test()"
      ],
      "metadata": {
        "id": "EY5mxbH96oEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}